{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n",
      "loading text...\n",
      "tokenizing sentences...\n",
      "creating/loading vocab...\n",
      "creating dataset...\n",
      "initializing model...\n",
      "initializing optimizer and loss...\n",
      "training...\n",
      "it: 0  | loss 10.4  | Δw: 2.423\n",
      "it: 10  | loss 8.74  | Δw: 0.773\n",
      "it: 20  | loss 8.3  | Δw: 0.343\n",
      "it: 30  | loss 8.12  | Δw: 0.26\n",
      "it: 40  | loss 8.07  | Δw: 0.204\n",
      "it: 50  | loss 7.68  | Δw: 0.174\n",
      "it: 60  | loss 7.63  | Δw: 0.165\n",
      "it: 70  | loss 7.34  | Δw: 0.154\n",
      "it: 80  | loss 7.15  | Δw: 0.144\n",
      "it: 90  | loss 7.03  | Δw: 0.147\n",
      "it: 100  | loss 6.8  | Δw: 0.132\n",
      "it: 110  | loss 6.54  | Δw: 0.13\n",
      "it: 120  | loss 6.43  | Δw: 0.122\n",
      "it: 130  | loss 6.32  | Δw: 0.125\n",
      "it: 140  | loss 6.18  | Δw: 0.119\n",
      "it: 150  | loss 5.96  | Δw: 0.112\n",
      "it: 160  | loss 5.87  | Δw: 0.113\n",
      "it: 170  | loss 5.8  | Δw: 0.115\n",
      "it: 180  | loss 5.73  | Δw: 0.11\n",
      "it: 190  | loss 5.55  | Δw: 0.108\n",
      "it: 200  | loss 5.58  | Δw: 0.111\n",
      "it: 210  | loss 5.5  | Δw: 0.109\n",
      "it: 220  | loss 5.28  | Δw: 0.104\n",
      "it: 230  | loss 5.29  | Δw: 0.105\n",
      "it: 240  | loss 5.23  | Δw: 0.109\n",
      "it: 250  | loss 5.23  | Δw: 0.107\n",
      "it: 260  | loss 5.15  | Δw: 0.102\n",
      "it: 270  | loss 4.92  | Δw: 0.108\n",
      "it: 280  | loss 5.04  | Δw: 0.112\n",
      "it: 290  | loss 4.94  | Δw: 0.117\n",
      "it: 300  | loss 4.96  | Δw: 0.116\n",
      "it: 310  | loss 4.89  | Δw: 0.116\n",
      "it: 320  | loss 4.87  | Δw: 0.117\n",
      "it: 330  | loss 4.9  | Δw: 0.119\n",
      "it: 340  | loss 4.81  | Δw: 0.12\n",
      "it: 350  | loss 4.82  | Δw: 0.127\n",
      "it: 360  | loss 4.81  | Δw: 0.13\n",
      "it: 370  | loss 4.81  | Δw: 0.147\n",
      "it: 380  | loss 4.8  | Δw: 0.166\n",
      "it: 390  | loss 4.87  | Δw: 0.157\n",
      "it: 400  | loss 4.8  | Δw: 0.172\n",
      "it: 410  | loss 4.77  | Δw: 0.182\n",
      "it: 420  | loss 4.8  | Δw: 0.192\n",
      "it: 430  | loss 4.74  | Δw: 0.195\n",
      "it: 440  | loss 4.71  | Δw: 0.242\n",
      "it: 450  | loss 4.78  | Δw: 0.263\n",
      "it: 460  | loss 4.77  | Δw: 0.283\n",
      "it: 470  | loss 4.74  | Δw: 0.294\n",
      "it: 480  | loss 4.79  | Δw: 0.359\n",
      "it: 490  | loss 4.69  | Δw: 0.352\n",
      "it: 500  | loss 4.72  | Δw: 0.409\n",
      "it: 510  | loss 4.77  | Δw: 0.431\n",
      "it: 520  | loss 4.77  | Δw: 0.486\n",
      "it: 530  | loss 4.67  | Δw: 0.473\n",
      "it: 540  | loss 4.66  | Δw: 0.539\n",
      "it: 550  | loss 4.84  | Δw: 0.574\n",
      "it: 560  | loss 4.65  | Δw: 0.64\n",
      "it: 570  | loss 4.53  | Δw: 0.689\n",
      "it: 580  | loss 4.66  | Δw: 0.795\n",
      "it: 590  | loss 4.52  | Δw: 0.784\n",
      "it: 600  | loss 4.59  | Δw: 0.882\n",
      "it: 610  | loss 4.62  | Δw: 0.812\n",
      "it: 620  | loss 4.72  | Δw: 0.873\n",
      "it: 630  | loss 4.64  | Δw: 0.832\n",
      "it: 640  | loss 4.56  | Δw: 0.956\n",
      "it: 650  | loss 4.65  | Δw: 0.876\n",
      "it: 660  | loss 4.63  | Δw: 1.096\n",
      "it: 670  | loss 4.68  | Δw: 1.175\n",
      "it: 680  | loss 4.59  | Δw: 1.076\n",
      "it: 690  | loss 4.54  | Δw: 1.149\n",
      "it: 700  | loss 4.4  | Δw: 1.242\n",
      "it: 710  | loss 4.52  | Δw: 1.359\n",
      "it: 720  | loss 4.52  | Δw: 1.379\n",
      "it: 730  | loss 4.57  | Δw: 1.584\n",
      "it: 740  | loss 4.55  | Δw: 1.549\n",
      "it: 750  | loss 4.56  | Δw: 1.589\n",
      "it: 760  | loss 4.62  | Δw: 1.529\n",
      "it: 770  | loss 4.57  | Δw: 1.709\n",
      "it: 780  | loss 4.45  | Δw: 1.597\n",
      "it: 790  | loss 4.51  | Δw: 1.732\n",
      "it: 800  | loss 4.48  | Δw: 1.801\n",
      "it: 810  | loss 4.44  | Δw: 1.987\n",
      "it: 820  | loss 4.54  | Δw: 1.974\n",
      "it: 830  | loss 4.49  | Δw: 2.158\n",
      "it: 840  | loss 4.48  | Δw: 2.024\n",
      "it: 850  | loss 4.43  | Δw: 2.11\n",
      "it: 860  | loss 4.42  | Δw: 2.332\n",
      "it: 870  | loss 4.49  | Δw: 2.381\n",
      "it: 880  | loss 4.36  | Δw: 2.34\n",
      "it: 890  | loss 4.47  | Δw: 2.405\n",
      "it: 900  | loss 4.42  | Δw: 2.48\n",
      "it: 910  | loss 4.36  | Δw: 2.674\n",
      "it: 920  | loss 4.51  | Δw: 2.871\n",
      "it: 930  | loss 4.39  | Δw: 2.649\n",
      "it: 940  | loss 4.39  | Δw: 2.651\n",
      "it: 950  | loss 4.29  | Δw: 3.562\n",
      "it: 960  | loss 4.31  | Δw: 2.899\n",
      "it: 970  | loss 4.39  | Δw: 2.791\n",
      "it: 980  | loss 4.26  | Δw: 2.839\n",
      "it: 990  | loss 4.35  | Δw: 2.906\n",
      "it: 1000  | loss 4.31  | Δw: 3.446\n",
      "it: 1010  | loss 4.23  | Δw: 3.208\n",
      "it: 1020  | loss 4.15  | Δw: 3.721\n",
      "it: 1030  | loss 4.28  | Δw: 3.428\n",
      "it: 1040  | loss 4.17  | Δw: 3.74\n",
      "it: 1050  | loss 4.29  | Δw: 3.724\n",
      "it: 1060  | loss 4.16  | Δw: 3.784\n",
      "it: 1070  | loss 4.19  | Δw: 3.684\n",
      "it: 1080  | loss 4.16  | Δw: 3.957\n",
      "it: 1090  | loss 4.27  | Δw: 4.061\n",
      "it: 1100  | loss 4.11  | Δw: 4.691\n",
      "it: 1110  | loss 4.21  | Δw: 4.293\n",
      "it: 1120  | loss 4.16  | Δw: 4.149\n",
      "it: 1130  | loss 4.15  | Δw: 4.247\n",
      "it: 1140  | loss 4.09  | Δw: 4.146\n",
      "it: 1150  | loss 4.07  | Δw: 4.027\n",
      "it: 1160  | loss 4.1  | Δw: 4.276\n",
      "it: 1170  | loss 4.15  | Δw: 4.069\n",
      "it: 1180  | loss 3.98  | Δw: 4.576\n",
      "it: 1190  | loss 4.05  | Δw: 4.434\n",
      "it: 1200  | loss 4.05  | Δw: 4.773\n",
      "it: 1210  | loss 4.08  | Δw: 4.781\n",
      "it: 1220  | loss 4.1  | Δw: 4.513\n",
      "it: 1230  | loss 4.16  | Δw: 5.066\n",
      "it: 1240  | loss 4.08  | Δw: 5.08\n",
      "it: 1250  | loss 4.09  | Δw: 5.299\n",
      "it: 1260  | loss 3.88  | Δw: 5.344\n",
      "it: 1270  | loss 3.95  | Δw: 5.246\n",
      "it: 1280  | loss 3.83  | Δw: 5.189\n",
      "it: 1290  | loss 3.98  | Δw: 5.513\n",
      "it: 1300  | loss 3.92  | Δw: 5.341\n",
      "it: 1310  | loss 3.91  | Δw: 5.192\n",
      "it: 1320  | loss 3.94  | Δw: 6.086\n",
      "it: 1330  | loss 3.93  | Δw: 5.747\n",
      "it: 1340  | loss 3.85  | Δw: 6.198\n",
      "it: 1350  | loss 3.85  | Δw: 6.011\n",
      "it: 1360  | loss 3.82  | Δw: 5.781\n",
      "it: 1370  | loss 3.94  | Δw: 6.572\n",
      "it: 1380  | loss 4.0  | Δw: 5.722\n",
      "it: 1390  | loss 3.96  | Δw: 5.729\n",
      "it: 1400  | loss 3.82  | Δw: 6.213\n",
      "it: 1410  | loss 3.74  | Δw: 6.954\n",
      "it: 1420  | loss 3.84  | Δw: 6.49\n",
      "it: 1430  | loss 3.74  | Δw: 6.497\n",
      "it: 1440  | loss 3.84  | Δw: 5.667\n",
      "it: 1450  | loss 3.72  | Δw: 6.136\n",
      "it: 1460  | loss 3.71  | Δw: 6.255\n",
      "it: 1470  | loss 3.75  | Δw: 6.817\n",
      "it: 1480  | loss 3.73  | Δw: 6.537\n",
      "it: 1490  | loss 3.79  | Δw: 6.279\n",
      "it: 1500  | loss 3.73  | Δw: 6.355\n",
      "it: 1510  | loss 3.84  | Δw: 6.861\n",
      "it: 1520  | loss 3.59  | Δw: 7.041\n",
      "it: 1530  | loss 3.79  | Δw: 6.916\n",
      "it: 1540  | loss 3.7  | Δw: 6.934\n",
      "it: 1550  | loss 3.66  | Δw: 6.862\n",
      "it: 1560  | loss 3.69  | Δw: 6.519\n",
      "it: 1570  | loss 3.7  | Δw: 7.092\n",
      "it: 1580  | loss 3.61  | Δw: 7.799\n",
      "it: 1590  | loss 3.68  | Δw: 7.56\n",
      "it: 1600  | loss 3.61  | Δw: 6.762\n",
      "it: 1610  | loss 3.71  | Δw: 7.322\n",
      "it: 1620  | loss 3.73  | Δw: 7.53\n",
      "it: 1630  | loss 3.6  | Δw: 7.332\n",
      "it: 1640  | loss 3.69  | Δw: 7.106\n",
      "it: 1650  | loss 3.54  | Δw: 7.616\n",
      "it: 1660  | loss 3.59  | Δw: 7.172\n",
      "it: 1670  | loss 3.57  | Δw: 7.462\n",
      "it: 1680  | loss 3.61  | Δw: 7.157\n",
      "it: 1690  | loss 3.48  | Δw: 7.764\n",
      "it: 1700  | loss 3.49  | Δw: 7.341\n",
      "it: 1710  | loss 3.49  | Δw: 8.003\n",
      "it: 1720  | loss 3.45  | Δw: 7.326\n",
      "it: 1730  | loss 3.52  | Δw: 7.794\n",
      "it: 1740  | loss 3.41  | Δw: 7.623\n",
      "it: 1750  | loss 3.44  | Δw: 7.921\n",
      "it: 1760  | loss 3.52  | Δw: 8.217\n",
      "it: 1770  | loss 3.47  | Δw: 8.169\n",
      "it: 1780  | loss 3.45  | Δw: 7.993\n",
      "it: 1790  | loss 3.58  | Δw: 8.488\n",
      "it: 1800  | loss 3.43  | Δw: 8.02\n",
      "it: 1810  | loss 3.42  | Δw: 8.164\n",
      "it: 1820  | loss 3.49  | Δw: 7.778\n",
      "it: 1830  | loss 3.34  | Δw: 8.835\n",
      "it: 1840  | loss 3.36  | Δw: 8.234\n",
      "it: 1850  | loss 3.38  | Δw: 8.237\n",
      "it: 1860  | loss 3.35  | Δw: 8.432\n",
      "it: 1870  | loss 3.36  | Δw: 9.066\n",
      "it: 1880  | loss 3.36  | Δw: 8.567\n",
      "it: 1890  | loss 3.27  | Δw: 8.37\n",
      "it: 1900  | loss 3.35  | Δw: 8.77\n",
      "it: 1910  | loss 3.27  | Δw: 9.128\n",
      "it: 1920  | loss 3.35  | Δw: 8.445\n",
      "it: 1930  | loss 3.44  | Δw: 8.619\n",
      "it: 1940  | loss 3.22  | Δw: 8.705\n",
      "it: 1950  | loss 3.32  | Δw: 8.465\n",
      "it: 1960  | loss 3.07  | Δw: 8.707\n",
      "it: 1970  | loss 3.15  | Δw: 9.184\n",
      "it: 1980  | loss 3.25  | Δw: 8.932\n",
      "it: 1990  | loss 3.13  | Δw: 8.781\n",
      "it: 2000  | loss 3.13  | Δw: 8.97\n",
      "it: 2010  | loss 3.23  | Δw: 9.064\n",
      "it: 2020  | loss 3.19  | Δw: 8.619\n",
      "it: 2030  | loss 3.25  | Δw: 8.925\n",
      "it: 2040  | loss 3.22  | Δw: 9.018\n",
      "it: 2050  | loss 3.15  | Δw: 9.321\n",
      "it: 2060  | loss 3.2  | Δw: 9.726\n",
      "it: 2070  | loss 3.17  | Δw: 9.267\n",
      "it: 2080  | loss 3.1  | Δw: 9.671\n",
      "it: 2090  | loss 3.17  | Δw: 9.147\n",
      "it: 2100  | loss 3.17  | Δw: 9.76\n",
      "it: 2110  | loss 3.0  | Δw: 9.508\n",
      "it: 2120  | loss 3.12  | Δw: 9.648\n",
      "it: 2130  | loss 3.02  | Δw: 9.795\n",
      "it: 2140  | loss 3.08  | Δw: 9.218\n",
      "it: 2150  | loss 3.13  | Δw: 9.338\n",
      "it: 2160  | loss 3.08  | Δw: 9.604\n",
      "it: 2170  | loss 3.05  | Δw: 9.431\n",
      "it: 2180  | loss 3.12  | Δw: 9.645\n",
      "it: 2190  | loss 3.1  | Δw: 9.995\n",
      "it: 2200  | loss 2.97  | Δw: 10.386\n",
      "it: 2210  | loss 2.94  | Δw: 9.74\n",
      "it: 2220  | loss 2.95  | Δw: 9.449\n",
      "it: 2230  | loss 2.98  | Δw: 10.334\n",
      "it: 2240  | loss 3.0  | Δw: 10.326\n",
      "it: 2250  | loss 3.07  | Δw: 9.462\n",
      "it: 2260  | loss 2.92  | Δw: 10.188\n",
      "it: 2270  | loss 2.92  | Δw: 9.784\n",
      "it: 2280  | loss 2.81  | Δw: 9.989\n",
      "it: 2290  | loss 2.81  | Δw: 10.319\n",
      "it: 2300  | loss 2.79  | Δw: 10.293\n",
      "it: 2310  | loss 2.88  | Δw: 10.261\n",
      "it: 2320  | loss 2.81  | Δw: 10.246\n",
      "it: 2330  | loss 2.82  | Δw: 10.381\n",
      "it: 2340  | loss 2.77  | Δw: 10.263\n",
      "it: 2350  | loss 2.84  | Δw: 10.319\n",
      "it: 2360  | loss 2.81  | Δw: 10.501\n",
      "it: 2370  | loss 2.86  | Δw: 10.523\n",
      "it: 2380  | loss 2.8  | Δw: 10.179\n",
      "it: 2390  | loss 2.74  | Δw: 9.8\n",
      "it: 2400  | loss 2.76  | Δw: 10.242\n",
      "it: 2410  | loss 2.77  | Δw: 10.395\n",
      "it: 2420  | loss 2.79  | Δw: 10.599\n",
      "it: 2430  | loss 2.79  | Δw: 10.659\n",
      "it: 2440  | loss 2.75  | Δw: 10.944\n",
      "it: 2450  | loss 2.87  | Δw: 10.592\n",
      "it: 2460  | loss 2.72  | Δw: 10.787\n",
      "it: 2470  | loss 2.73  | Δw: 11.761\n",
      "it: 2480  | loss 2.66  | Δw: 11.13\n",
      "it: 2490  | loss 2.8  | Δw: 10.978\n",
      "it: 2500  | loss 2.54  | Δw: 10.846\n",
      "it: 2510  | loss 2.67  | Δw: 11.089\n",
      "it: 2520  | loss 2.82  | Δw: 10.694\n",
      "it: 2530  | loss 2.62  | Δw: 10.401\n",
      "it: 2540  | loss 2.74  | Δw: 10.871\n",
      "it: 2550  | loss 2.59  | Δw: 11.037\n",
      "it: 2560  | loss 2.56  | Δw: 11.074\n",
      "it: 2570  | loss 2.57  | Δw: 11.144\n",
      "it: 2580  | loss 2.62  | Δw: 11.069\n",
      "it: 2590  | loss 2.65  | Δw: 11.223\n",
      "it: 2600  | loss 2.61  | Δw: 11.338\n",
      "it: 2610  | loss 2.62  | Δw: 11.506\n",
      "it: 2620  | loss 2.57  | Δw: 11.145\n",
      "it: 2630  | loss 2.6  | Δw: 11.945\n",
      "it: 2640  | loss 2.54  | Δw: 11.168\n",
      "it: 2650  | loss 2.69  | Δw: 11.354\n",
      "it: 2660  | loss 2.57  | Δw: 11.453\n",
      "it: 2670  | loss 2.55  | Δw: 12.416\n",
      "it: 2680  | loss 2.57  | Δw: 11.79\n",
      "it: 2690  | loss 2.55  | Δw: 12.357\n",
      "it: 2700  | loss 2.47  | Δw: 11.933\n",
      "it: 2710  | loss 2.6  | Δw: 11.376\n",
      "it: 2720  | loss 2.4  | Δw: 11.286\n",
      "it: 2730  | loss 2.55  | Δw: 11.772\n",
      "it: 2740  | loss 2.38  | Δw: 11.174\n",
      "it: 2750  | loss 2.51  | Δw: 12.32\n",
      "it: 2760  | loss 2.41  | Δw: 11.621\n",
      "it: 2770  | loss 2.37  | Δw: 11.809\n",
      "it: 2780  | loss 2.42  | Δw: 11.877\n",
      "it: 2790  | loss 2.42  | Δw: 12.092\n",
      "it: 2800  | loss 2.42  | Δw: 12.313\n",
      "it: 2810  | loss 2.35  | Δw: 11.7\n",
      "it: 2820  | loss 2.39  | Δw: 11.806\n",
      "it: 2830  | loss 2.4  | Δw: 11.756\n",
      "it: 2840  | loss 2.36  | Δw: 12.371\n",
      "it: 2850  | loss 2.35  | Δw: 12.599\n",
      "it: 2860  | loss 2.39  | Δw: 12.289\n",
      "it: 2870  | loss 2.47  | Δw: 12.034\n",
      "it: 2880  | loss 2.16  | Δw: 11.94\n",
      "it: 2890  | loss 2.32  | Δw: 12.14\n",
      "it: 2900  | loss 2.26  | Δw: 12.896\n",
      "it: 2910  | loss 2.3  | Δw: 12.416\n",
      "it: 2920  | loss 2.28  | Δw: 12.319\n",
      "it: 2930  | loss 2.36  | Δw: 11.992\n",
      "it: 2940  | loss 2.24  | Δw: 12.273\n",
      "it: 2950  | loss 2.34  | Δw: 12.426\n",
      "it: 2960  | loss 2.35  | Δw: 12.941\n",
      "it: 2970  | loss 2.23  | Δw: 12.724\n",
      "it: 2980  | loss 2.26  | Δw: 12.076\n",
      "it: 2990  | loss 2.29  | Δw: 13.05\n",
      "it: 3000  | loss 2.26  | Δw: 13.189\n",
      "it: 3010  | loss 2.15  | Δw: 12.211\n",
      "it: 3020  | loss 2.21  | Δw: 12.149\n",
      "it: 3030  | loss 2.17  | Δw: 12.442\n",
      "it: 3040  | loss 2.15  | Δw: 12.793\n",
      "it: 3050  | loss 2.25  | Δw: 12.625\n",
      "it: 3060  | loss 2.26  | Δw: 13.776\n",
      "it: 3070  | loss 2.28  | Δw: 13.368\n",
      "it: 3080  | loss 2.16  | Δw: 12.367\n",
      "it: 3090  | loss 2.21  | Δw: 12.541\n",
      "it: 3100  | loss 2.15  | Δw: 12.662\n",
      "it: 3110  | loss 2.18  | Δw: 12.784\n",
      "it: 3120  | loss 2.18  | Δw: 12.147\n",
      "it: 3130  | loss 2.08  | Δw: 12.529\n",
      "it: 3140  | loss 2.04  | Δw: 12.322\n",
      "it: 3150  | loss 2.07  | Δw: 12.022\n",
      "it: 3160  | loss 2.11  | Δw: 12.446\n",
      "it: 3170  | loss 2.06  | Δw: 12.754\n",
      "it: 3180  | loss 2.12  | Δw: 12.69\n",
      "it: 3190  | loss 2.16  | Δw: 13.085\n",
      "it: 3200  | loss 2.16  | Δw: 12.888\n",
      "it: 3210  | loss 2.08  | Δw: 13.089\n",
      "it: 3220  | loss 2.04  | Δw: 13.518\n",
      "it: 3230  | loss 2.04  | Δw: 12.944\n",
      "it: 3240  | loss 2.12  | Δw: 13.381\n",
      "it: 3250  | loss 2.02  | Δw: 13.527\n",
      "it: 3260  | loss 1.94  | Δw: 12.592\n",
      "it: 3270  | loss 1.99  | Δw: 12.987\n",
      "it: 3280  | loss 2.06  | Δw: 12.91\n",
      "it: 3290  | loss 1.89  | Δw: 12.566\n",
      "it: 3300  | loss 2.01  | Δw: 13.052\n",
      "it: 3310  | loss 2.03  | Δw: 13.207\n",
      "it: 3320  | loss 2.02  | Δw: 13.072\n",
      "it: 3330  | loss 2.0  | Δw: 13.088\n",
      "it: 3340  | loss 1.98  | Δw: 13.277\n",
      "it: 3350  | loss 1.92  | Δw: 13.292\n",
      "it: 3360  | loss 1.89  | Δw: 13.12\n",
      "it: 3370  | loss 1.93  | Δw: 13.195\n",
      "it: 3380  | loss 1.92  | Δw: 13.501\n",
      "it: 3390  | loss 1.9  | Δw: 13.132\n",
      "it: 3400  | loss 1.99  | Δw: 13.284\n",
      "it: 3410  | loss 2.04  | Δw: 14.197\n",
      "it: 3420  | loss 1.99  | Δw: 13.48\n",
      "it: 3430  | loss 1.89  | Δw: 13.205\n",
      "it: 3440  | loss 1.87  | Δw: 13.259\n",
      "it: 3450  | loss 1.99  | Δw: 14.192\n",
      "it: 3460  | loss 1.85  | Δw: 13.222\n",
      "it: 3470  | loss 1.79  | Δw: 14.301\n",
      "it: 3480  | loss 1.92  | Δw: 12.978\n",
      "it: 3490  | loss 1.92  | Δw: 13.193\n",
      "it: 3500  | loss 1.87  | Δw: 13.455\n",
      "it: 3510  | loss 1.84  | Δw: 12.986\n",
      "it: 3520  | loss 1.76  | Δw: 13.405\n",
      "it: 3530  | loss 1.91  | Δw: 14.219\n",
      "it: 3540  | loss 1.81  | Δw: 12.904\n",
      "it: 3550  | loss 1.73  | Δw: 13.866\n",
      "it: 3560  | loss 1.87  | Δw: 13.549\n",
      "it: 3570  | loss 1.79  | Δw: 13.71\n",
      "it: 3580  | loss 1.78  | Δw: 13.562\n",
      "it: 3590  | loss 1.77  | Δw: 13.02\n",
      "it: 3600  | loss 1.74  | Δw: 13.865\n",
      "it: 3610  | loss 1.77  | Δw: 13.813\n",
      "it: 3620  | loss 1.84  | Δw: 13.546\n",
      "it: 3630  | loss 1.72  | Δw: 13.447\n",
      "it: 3640  | loss 1.75  | Δw: 13.442\n",
      "it: 3650  | loss 1.71  | Δw: 13.662\n",
      "it: 3660  | loss 1.71  | Δw: 13.44\n",
      "it: 3670  | loss 1.85  | Δw: 13.941\n",
      "it: 3680  | loss 1.67  | Δw: 13.896\n",
      "it: 3690  | loss 1.75  | Δw: 13.9\n",
      "it: 3700  | loss 1.73  | Δw: 14.462\n",
      "it: 3710  | loss 1.68  | Δw: 13.62\n",
      "it: 3720  | loss 1.68  | Δw: 14.467\n",
      "it: 3730  | loss 1.69  | Δw: 13.91\n",
      "it: 3740  | loss 1.63  | Δw: 13.752\n",
      "it: 3750  | loss 1.8  | Δw: 13.851\n",
      "it: 3760  | loss 1.71  | Δw: 14.852\n",
      "it: 3770  | loss 1.76  | Δw: 13.992\n",
      "it: 3780  | loss 1.67  | Δw: 14.971\n",
      "it: 3790  | loss 1.68  | Δw: 14.071\n",
      "it: 3800  | loss 1.71  | Δw: 14.288\n",
      "it: 3810  | loss 1.67  | Δw: 14.256\n",
      "it: 3820  | loss 1.59  | Δw: 14.06\n",
      "it: 3830  | loss 1.55  | Δw: 14.321\n",
      "it: 3840  | loss 1.63  | Δw: 14.782\n",
      "it: 3850  | loss 1.67  | Δw: 13.976\n",
      "it: 3860  | loss 1.58  | Δw: 13.405\n",
      "it: 3870  | loss 1.59  | Δw: 14.187\n",
      "it: 3880  | loss 1.66  | Δw: 13.972\n",
      "it: 3890  | loss 1.54  | Δw: 13.992\n",
      "it: 3900  | loss 1.62  | Δw: 13.962\n",
      "it: 3910  | loss 1.53  | Δw: 14.661\n",
      "it: 3920  | loss 1.54  | Δw: 14.345\n",
      "it: 3930  | loss 1.64  | Δw: 14.498\n",
      "it: 3940  | loss 1.58  | Δw: 14.681\n",
      "it: 3950  | loss 1.54  | Δw: 14.488\n",
      "it: 3960  | loss 1.57  | Δw: 13.955\n",
      "it: 3970  | loss 1.44  | Δw: 13.409\n",
      "it: 3980  | loss 1.52  | Δw: 14.815\n",
      "it: 3990  | loss 1.52  | Δw: 14.185\n",
      "it: 4000  | loss 1.54  | Δw: 14.809\n",
      "it: 4010  | loss 1.46  | Δw: 14.228\n",
      "it: 4020  | loss 1.49  | Δw: 14.494\n",
      "it: 4030  | loss 1.49  | Δw: 15.049\n",
      "it: 4040  | loss 1.54  | Δw: 15.874\n",
      "it: 4050  | loss 1.37  | Δw: 14.241\n",
      "it: 4060  | loss 1.57  | Δw: 14.906\n",
      "it: 4070  | loss 1.5  | Δw: 14.559\n",
      "it: 4080  | loss 1.49  | Δw: 14.367\n",
      "it: 4090  | loss 1.46  | Δw: 14.233\n",
      "it: 4100  | loss 1.45  | Δw: 13.972\n",
      "it: 4110  | loss 1.43  | Δw: 14.255\n",
      "it: 4120  | loss 1.4  | Δw: 14.417\n",
      "it: 4130  | loss 1.43  | Δw: 14.272\n",
      "it: 4140  | loss 1.36  | Δw: 14.563\n",
      "it: 4150  | loss 1.51  | Δw: 14.317\n",
      "it: 4160  | loss 1.48  | Δw: 14.894\n",
      "it: 4170  | loss 1.43  | Δw: 14.459\n",
      "it: 4180  | loss 1.51  | Δw: 14.984\n",
      "it: 4190  | loss 1.51  | Δw: 14.398\n",
      "it: 4200  | loss 1.41  | Δw: 14.663\n",
      "it: 4210  | loss 1.37  | Δw: 15.057\n",
      "it: 4220  | loss 1.34  | Δw: 14.111\n",
      "it: 4230  | loss 1.4  | Δw: 14.562\n",
      "it: 4240  | loss 1.39  | Δw: 14.417\n",
      "it: 4250  | loss 1.3  | Δw: 14.442\n",
      "it: 4260  | loss 1.44  | Δw: 14.449\n",
      "it: 4270  | loss 1.3  | Δw: 14.318\n",
      "it: 4280  | loss 1.36  | Δw: 14.728\n",
      "it: 4290  | loss 1.39  | Δw: 14.408\n",
      "it: 4300  | loss 1.43  | Δw: 15.389\n",
      "it: 4310  | loss 1.34  | Δw: 14.147\n",
      "it: 4320  | loss 1.39  | Δw: 14.453\n",
      "it: 4330  | loss 1.44  | Δw: 14.545\n",
      "it: 4340  | loss 1.31  | Δw: 14.432\n",
      "it: 4350  | loss 1.31  | Δw: 14.647\n",
      "it: 4360  | loss 1.29  | Δw: 14.252\n",
      "it: 4370  | loss 1.28  | Δw: 14.54\n",
      "it: 4380  | loss 1.37  | Δw: 14.964\n",
      "it: 4390  | loss 1.35  | Δw: 14.365\n",
      "it: 4400  | loss 1.3  | Δw: 14.38\n",
      "it: 4410  | loss 1.36  | Δw: 14.624\n",
      "it: 4420  | loss 1.2  | Δw: 14.943\n",
      "it: 4430  | loss 1.22  | Δw: 14.603\n",
      "it: 4440  | loss 1.27  | Δw: 14.162\n",
      "it: 4450  | loss 1.29  | Δw: 14.651\n",
      "it: 4460  | loss 1.23  | Δw: 14.641\n",
      "it: 4470  | loss 1.29  | Δw: 14.692\n",
      "it: 4480  | loss 1.26  | Δw: 15.17\n",
      "it: 4490  | loss 1.24  | Δw: 15.236\n",
      "it: 4500  | loss 1.31  | Δw: 14.92\n",
      "it: 4510  | loss 1.28  | Δw: 14.998\n",
      "it: 4520  | loss 1.19  | Δw: 15.096\n",
      "it: 4530  | loss 1.22  | Δw: 14.739\n",
      "it: 4540  | loss 1.23  | Δw: 14.888\n",
      "it: 4550  | loss 1.2  | Δw: 14.951\n",
      "it: 4560  | loss 1.18  | Δw: 14.514\n",
      "it: 4570  | loss 1.15  | Δw: 14.801\n",
      "it: 4580  | loss 1.25  | Δw: 14.713\n",
      "it: 4590  | loss 1.27  | Δw: 16.126\n",
      "it: 4600  | loss 1.18  | Δw: 14.612\n",
      "it: 4610  | loss 1.14  | Δw: 15.507\n",
      "it: 4620  | loss 1.24  | Δw: 15.199\n",
      "it: 4630  | loss 1.12  | Δw: 14.648\n",
      "it: 4640  | loss 1.26  | Δw: 14.934\n",
      "it: 4650  | loss 1.2  | Δw: 14.573\n",
      "it: 4660  | loss 1.13  | Δw: 14.801\n",
      "it: 4670  | loss 1.2  | Δw: 14.88\n",
      "it: 4680  | loss 1.18  | Δw: 15.037\n",
      "it: 4690  | loss 1.17  | Δw: 15.502\n",
      "it: 4700  | loss 1.16  | Δw: 15.323\n",
      "it: 4710  | loss 1.13  | Δw: 14.764\n",
      "it: 4720  | loss 1.11  | Δw: 15.008\n",
      "it: 4730  | loss 1.18  | Δw: 15.135\n",
      "it: 4740  | loss 1.16  | Δw: 16.083\n",
      "it: 4750  | loss 1.07  | Δw: 14.52\n",
      "it: 4760  | loss 1.09  | Δw: 14.608\n",
      "it: 4770  | loss 1.08  | Δw: 14.882\n",
      "it: 4780  | loss 1.06  | Δw: 14.755\n",
      "it: 4790  | loss 1.11  | Δw: 14.593\n",
      "it: 4800  | loss 1.16  | Δw: 15.264\n",
      "it: 4810  | loss 0.99  | Δw: 14.664\n",
      "it: 4820  | loss 1.09  | Δw: 14.756\n",
      "it: 4830  | loss 1.06  | Δw: 15.204\n",
      "it: 4840  | loss 1.11  | Δw: 15.187\n",
      "it: 4850  | loss 1.06  | Δw: 15.528\n",
      "it: 4860  | loss 1.09  | Δw: 15.078\n",
      "it: 4870  | loss 1.06  | Δw: 15.23\n",
      "it: 4880  | loss 1.05  | Δw: 15.682\n",
      "it: 4890  | loss 1.04  | Δw: 15.043\n",
      "it: 4900  | loss 1.07  | Δw: 15.391\n",
      "it: 4910  | loss 1.04  | Δw: 14.418\n",
      "it: 4920  | loss 1.03  | Δw: 15.374\n",
      "it: 4930  | loss 1.1  | Δw: 15.351\n",
      "it: 4940  | loss 1.03  | Δw: 14.755\n",
      "it: 4950  | loss 1.09  | Δw: 15.712\n",
      "it: 4960  | loss 1.02  | Δw: 14.701\n",
      "it: 4970  | loss 0.97  | Δw: 15.126\n",
      "it: 4980  | loss 1.1  | Δw: 15.595\n",
      "it: 4990  | loss 1.05  | Δw: 14.525\n",
      "saving embeddings...\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Libs\n",
    "# =============================================================================\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Transformer\n",
    "# =============================================================================\n",
    "def attention(q, k, v, mask = None, dropout = None):\n",
    "    scores = q.matmul(k.transpose(-2, -1))\n",
    "    scores /= math.sqrt(q.shape[-1])\n",
    "    \n",
    "    #mask\n",
    "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    scores = dropout(scores) if dropout is not None else scores\n",
    "    output = scores.matmul(v)\n",
    "    return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
    "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim_per_head = out_dim // n_heads\n",
    "        self.out = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, t):\n",
    "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
    "    \n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        #in decoder, y comes from encoder. In encoder, y=x\n",
    "        y = x if y is None else y\n",
    "        \n",
    "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        \n",
    "        #break into n_heads\n",
    "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        \n",
    "        #n_heads => attention => merge the heads => mix information\n",
    "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #inp => inner => relu => dropout => inner => inp\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
    "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
    "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #model input\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
    "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
    "        \n",
    "        #backbone\n",
    "        encoders = []\n",
    "        for i in range(n_code):\n",
    "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        \n",
    "        #language model\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x + self.pe(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Positional Embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe.requires_grad = False\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n",
    "    \n",
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class SentencesDataset(Dataset):\n",
    "    #Init dataset\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "        \n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
    "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "        \n",
    "        #special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
    "    \n",
    "    \n",
    "    #fetch data\n",
    "    def __getitem__(self, index, p_random_mask=0.15):\n",
    "        dataset = self\n",
    "        \n",
    "        #while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "        \n",
    "        #ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
    "        \n",
    "        #apply random mask\n",
    "        s = [(dataset.MASK_IDX, w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
    "        \n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "    #return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "        return s\n",
    "\n",
    "# =============================================================================\n",
    "# Methods / Class\n",
    "# =============================================================================\n",
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter\n",
    "\n",
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 512\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n",
    "\n",
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'training.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "print('initializing model...')\n",
    "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
    "\n",
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 5000\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    \n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss \n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [dataset.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
    "\n",
    "\n",
    "print('end')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
