{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspace/assignment_11_everything/S11Class/GPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (38193 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 89.48M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import Transformer\n",
    "from transformers import AutoTokenizer  # pip install transformers\n",
    "from utils import (\n",
    "    BATCH_SIZE,\n",
    "    BLOCK_SIZE,\n",
    "    DEVICE,\n",
    "    DROPOUT,\n",
    "    LEARNING_RATE,\n",
    "    NUM_EMBED,\n",
    "    NUM_HEAD,\n",
    "    NUM_LAYER,\n",
    "    MAX_ITER,\n",
    "    EVAL_INTER,\n",
    "    encode,\n",
    "    decode,\n",
    "    get_batch,\n",
    "    save_model_to_chekpoint,\n",
    "    estimate_loss,\n",
    ")\n",
    "\n",
    "# load model from checkpoint\n",
    "# m = load_model_from_checkpoint(Transformer,vocab_size=vocab_size)\n",
    "\n",
    "# example to decode sequence\n",
    "# enc_sec = m.generate(idx=torch.zeros((1,1), dtype=torch.long),\n",
    "# max_new_tokens=20)[0].tolist()\n",
    "# print(decode(vocab=vocab, enc_sec=enc_sec))\n",
    "\n",
    "# raw data\n",
    "path_do_data = \"data/english.txt\"\n",
    "data_raw = open(path_do_data, encoding=\"utf-8\").read()\n",
    "# we use pretrained BERT tokenizer for performance improvements\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "# data_raw = data_raw[4000000:] # short dataset\n",
    "\n",
    "# train/val split\n",
    "data = encode(text_seq=data_raw, tokenizer=tokenizer)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# train a new model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_embed=NUM_EMBED,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    num_heads=NUM_HEAD,\n",
    "    num_layers=NUM_LAYER,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "# load model to GPU if available\n",
    "m = model.to(DEVICE)\n",
    "# print the number of parameters in the model\n",
    "print(\n",
    "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step          0 | train loss 10.7364 | val loss 10.7221\n",
      "step        500 | train loss 0.4247 | val loss 10.3520\n",
      "step       1000 | train loss 0.1453 | val loss 11.7933\n",
      "step       1500 | train loss 0.1303 | val loss 12.2943\n",
      "step       2000 | train loss 0.1156 | val loss 12.6118\n",
      "step       2500 | train loss 0.1101 | val loss 12.8780\n",
      "step       3000 | train loss 0.1162 | val loss 13.1315\n",
      "step       3500 | train loss 0.1086 | val loss 13.6703\n",
      "step       4000 | train loss 0.0989 | val loss 13.5008\n",
      "step       4500 | train loss 0.1073 | val loss 13.4774\n",
      "step       4999 | train loss 0.0983 | val loss 13.7215\n"
     ]
    }
   ],
   "source": [
    "# optimizer takes the model's parameters and the learning rate as input,\n",
    "# and updates the parameters during the training process in order to\n",
    "# minimize the loss function.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
    "MAX_ITER = 5000\n",
    "for step in range(MAX_ITER):\n",
    "\n",
    "    # every EVAL_INTER evaluate the loss on train and val sets\n",
    "    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n",
    "        loss_train = estimate_loss(\n",
    "            data=train_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        loss_val = estimate_loss(\n",
    "            data=val_data, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data=train_data, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE)\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # backward() method on the loss variable calculates the gradients \n",
    "    # of the loss with respect to the model's parameters.\n",
    "    loss.backward()\n",
    "    # step() method on the optimizer updates the model's parameters \n",
    "    # using the calculated gradients, in order to minimize the loss.\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model to checkpoint/checkpoint_epoch-4999_07.04.2023_20:18:15.pt\n"
     ]
    }
   ],
   "source": [
    "save_model_to_chekpoint(model=m, path_to_checkpoint=\"checkpoint\", epoch=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] of the network, the weights can be adjusted in order to improve its performance. architecture backpropagation, a supervised algorithm first applied to machine learning systems in paul werbos'1974 dissertation, efficiently calculates \" gradients \", which are vector fields describing the optimal adjustment of all weights in the entire network for a given input / output example. the use of these gradients to train neural networks, a practice known as gradient descent, enabled the creation of much more complex systems, and wide\n",
      "[PAD] classification \" in a language understanding by generative pre - training, in which they introduced the generative pre - trained transformer ( gpt ). at that point, the best - performing neural nlp models mostly employed supervised learning from large amounts of manually labeled data. this reliance on supervised learning limited their use on datasets that were not well - annotated, and also made it prohibitively expensive and time - consuming to train extremely large models. many languages ( such as swahili\n",
      "[PAD]ncy that case it is max operation that calculates the maximum of the activations of the units in its patch. max - pooling is often used in modern cnns. local connectivity several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. today, however, the cnn architecture is usually trained through backpropagation. spatial arrangement the neocognitron is the first cnns will learn the convolution kernel\n",
      "[PAD]versarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. such models are not classifiers. discriminative models in application to classification, the observable x is frequently a continuous variable, the target y is generally a discrete variable consisting of a finite set of labels, and the conditional probability p ( [UNK] ) { \\ displaystyle p ( y \\ mid x ) } can also be interpreted as a ( non\n",
      "[PAD]prop calculates the errors between calculated output and sample output data, also known as a supervisory signal. in the mathematical model, each training example is represented by an array or vector, and the bi - directional the use a series of weights. the sum of the products of the weights and the inputs is calculated in each node. the mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. this technique has been known for over fewer parameters than\n",
      "[PAD] \" toronto \", published by george cybenko for sigmoid activation functions and was generalised to feed - forward multi - layer architectures in 1991 by kurt hornik. recent work also showed that universal approximation also holds for non - bounded activation functions such as the rectified xeon phi. dropconnect in the past, traditional multilayer perceptron ( mlp ) models were used for image recognition. however, the full connectivity between nodes caused the curse of dimensionality\n",
      "[PAD] classification \" is done via fully connected layer. neurons in a series forecasting another important concept of cnns is pooling, which networks important nested differentiable functions. in 1973, dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. werbos's ( 1975 ) backpropagation algorithm enabled practical training of multi - layer networks. in 1982, he applied linnainmaa's ad method to neural networks in the way that\n",
      "[PAD] or â„“2 - norm pooling. average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice. human interpretable explanations due to the effects of fast spatial reduction of the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether. related architectures \" region of interest \" pooling ( also known as roi pooling ) is a variant of max pooling, in which output\n",
      "[PAD] systems. max on investigating the classification algorithm that filters emails, which uses attention in place of previous recurrence - and convolution - based architectures. attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. this model allows for greatly increased parallelization, and outperforms previous benchmarks for rnn / cnn / lstm - based models. computational linguistics openai released the complete version of the gpt - 2 language\n",
      "[PAD] \" classify \" future \" input features. it has been discover clear if the term club is related to the word sense of a club sandwich, baseball club, clubhouse, golf club, or any other sense that club might have. the necessity to accommodate multiple meanings per word in different vectors ( multi - sense embeddings ) is the motivation for several contributions in nlp to split single - sense embeddings into multi - sense ones. see also most approaches that produce multi - sense embed\n",
      "[PAD] classification is done via fully connected layers. neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular ( non - convolutional ) artificial neural networks. their activations can thus be computed as the neuron takes input from a larger area in the input than previous layers. this is due to applying the convolution over and over, which takes into account the value of a pixel, as well as its surrounding pixels. when using dil\n"
     ]
    }
   ],
   "source": [
    "# generate some output based on the context\n",
    "for i in range(0,11):\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "    print(\n",
    "        decode(\n",
    "            enc_sec=m.generate(idx=context, max_new_tokens=100, block_size=BLOCK_SIZE)[0],\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
